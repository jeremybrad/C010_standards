# Project Relationships & Data Flow

**Last Updated:** 2025-11-19
**Maintained By:** Jeremy Bradford & Claude

This document maps how data flows through Jeremy's memory, knowledge, and work systems. Use this to understand dependencies before making changes.

---

## Quick Navigation

- [Memory System Architecture](#memory-system-architecture) - SADB â†’ CBFS â†’ MyBuddy pipeline
- [Visual Diagrams](MEMORY_ARCHITECTURE_DIAGRAMS.md) - **ðŸŽ¨ See Mermaid flowcharts and system maps**
- [Additional Memory Systems](#additional-memory-systems) - P159, P190, Terminal Insights, MCP Memory
- [Work Systems Integration](#work-systems-integration) - How memory feeds into analytics
- [Critical Dependencies](#critical-dependencies) - What depends on what
- [For LLMs: Finding Canonical Versions](#for-llms-finding-canonical-versions) - How to avoid grabbing old files

---

## Memory System Architecture

### The Canonical Pipeline (Multi-Stage Refinement)

This is the core data flow for processing conversations into structured knowledge:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 0: CONVERSATION SOURCES                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ ChatGPT exports (conversations.json in .zip files)           â”‚
â”‚  â€¢ Claude exports (from claude.ai)                              â”‚
â”‚  â€¢ Council logs (terminal interaction history)                  â”‚
â”‚  â€¢ Location: ~/Downloads â†’ $SADB_DATA_DIR/conversation-exports/ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ C002_sadb: SADB CORE PROCESSING                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Purpose: Initial extraction and consensus validation           â”‚
â”‚                                                                  â”‚
â”‚  Process:                                                        â”‚
â”‚   1. Chunking (semantic + sliding window)                       â”‚
â”‚      â€¢ 1.5k tokens per chunk, 400 token overlap                 â”‚
â”‚   2. 3-Pass Consensus Extraction                                â”‚
â”‚      â€¢ Local LLMs via Ollama/vLLM/LM Studio                     â”‚
â”‚      â€¢ Multi-model agreement required                           â”‚
â”‚   3. Validation & Enrichment                                    â”‚
â”‚      â€¢ Speaker attribution                                      â”‚
â”‚      â€¢ Metadata overlay                                         â”‚
â”‚   4. Human Review Queue                                         â”‚
â”‚      â€¢ Web UI or CLI for approval                               â”‚
â”‚                                                                  â”‚
â”‚  Outputs:                                                        â”‚
â”‚   â€¢ SQLite canonical store (entries, runs, contradictions)      â”‚
â”‚   â€¢ twin_feed_v1.ndjson (facts + preferences only)              â”‚
â”‚   â€¢ Location: $SADB_DATA_DIR/twin/                              â”‚
â”‚                                                                  â”‚
â”‚  Data Policy: All artifacts external to git ($SADB_DATA_DIR)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ C003_sadb_canonical: REFINEMENT PIPELINE (Stages S0-S2, M1-B1) â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Purpose: Deterministic normalization and multi-stage enrichmentâ”‚
â”‚  Status: Production (v0.2) with GPU acceleration                â”‚
â”‚                                                                  â”‚
â”‚  STAGE S0 - CAPTURE:                                            â”‚
â”‚   â€¢ Dry-run ingest with SHA256 deduplication                    â”‚
â”‚   â€¢ Receipt generation for provenance                           â”‚
â”‚   â€¢ Search ledger maintenance                                   â”‚
â”‚                                                                  â”‚
â”‚  STAGE S1 - NORMALIZE:                                          â”‚
â”‚   â€¢ Schema validation (JSON structure)                          â”‚
â”‚   â€¢ Leak scanning (PII detection)                               â”‚
â”‚   â€¢ Data sanitization                                           â”‚
â”‚   â€¢ Output: 95,266 messages across 601 conversations            â”‚
â”‚                                                                  â”‚
â”‚  STAGE S2 - ENRICH:                                             â”‚
â”‚   â€¢ Semantic windowing (context preservation)                   â”‚
â”‚   â€¢ BERTopic GPU clustering (RTX 5080, 527x speedup!)          â”‚
â”‚   â€¢ Semantic filtering (Topic 0 reduced from 22% â†’ 0.46%)      â”‚
â”‚   â€¢ Embedding generation (vector representations)               â”‚
â”‚   â€¢ Output: 70,539 enriched windows                             â”‚
â”‚                                                                  â”‚
â”‚  STAGE M1 - Q/A MEMORY:                                         â”‚
â”‚   â€¢ Extract Q/A pairs from normalized messages                  â”‚
â”‚   â€¢ Purpose: Feed memory systems (CBFS, MyBuddy, digital twin)  â”‚
â”‚   â€¢ Output: 4,092 Q/A pairs from 601 conversations              â”‚
â”‚   â€¢ Contract: 10_docs/STAGE_M1_QA_CONTRACT.md                   â”‚
â”‚                                                                  â”‚
â”‚  STAGE U1 - UTTERANCE SEGMENTATION:                             â”‚
â”‚   â€¢ Segment M1 Q/A pairs into atomic units                      â”‚
â”‚   â€¢ Flag code blocks, log entries, questions                    â”‚
â”‚   â€¢ Purpose: Fine-grained fact extraction units                 â”‚
â”‚   â€¢ Output: 108,487 segments with metadata flags                â”‚
â”‚   â€¢ Contract: 10_docs/STAGE_U1_SEGMENTS_CONTRACT.md             â”‚
â”‚                                                                  â”‚
â”‚  STAGE J1 - FACT CANDIDATES:                                    â”‚
â”‚   â€¢ LLM-based fact extraction from U1 segments                  â”‚
â”‚   â€¢ Format: Subject-predicate-object triples                    â”‚
â”‚   â€¢ Includes certainty/stance scoring                           â”‚
â”‚   â€¢ Test run: 7 facts from 20 segments (v0)                     â”‚
â”‚   â€¢ Web import (Nov 2025): 508 merged facts                     â”‚
â”‚   â€¢ Contract: 10_docs/STAGE_J1_FACT_CANDIDATES_CONTRACT.md      â”‚
â”‚                                                                  â”‚
â”‚  STAGE B1 - BRAIDED TIMELINE:                                   â”‚
â”‚   â€¢ Merge S1 chat messages + P181 terminal commands             â”‚
â”‚   â€¢ Creates unified event timeline                              â”‚
â”‚   â€¢ Shows what Jeremy discussed + what he executed              â”‚
â”‚   â€¢ Output: 95,266 events (Sep 2023 â†’ Oct 2025)                â”‚
â”‚   â€¢ Contract: 10_docs/STAGE_B1_BRAIDED_TIMELINE_CONTRACT.md     â”‚
â”‚   â€¢ Integration: ~/SyncedProjects/P181_terminal-insights/       â”‚
â”‚                  docs/SADB_INTEGRATION_BRAIDING.md              â”‚
â”‚                                                                  â”‚
â”‚  STAGE J1â†”B1 PROXIMITY JOIN:                                    â”‚
â”‚   â€¢ Link J1 facts to nearby B1 timeline events                  â”‚
â”‚   â€¢ Uses Â±15min temporal window (capped at 10 events)          â”‚
â”‚   â€¢ Enables query: "What was Jeremy doing when this fact emerged?"â”‚
â”‚   â€¢ Result: 282/282 facts linked, avg 9.86 events per fact      â”‚
â”‚   â€¢ Receipt: 20_receipts/J1B1_PROXIMITY_COMPLETE_20251115.md    â”‚
â”‚                                                                  â”‚
â”‚  âš ï¸ STAGES M2/M3 - FUTURE/PLANNED:                             â”‚
â”‚   â€¢ M2: [Purpose TBD - Memory consolidation/synthesis?]         â”‚
â”‚   â€¢ M3: [Purpose TBD - Memory validation/integration?]          â”‚
â”‚   â€¢ TODO: Document these stages when implemented                â”‚
â”‚                                                                  â”‚
â”‚  Key Artifacts:                                                  â”‚
â”‚   â€¢ GPU Environment: WSL Ubuntu, bertopic-blackwell conda env   â”‚
â”‚   â€¢ Filtered Corpus: 60_artifacts/filtered/ (70,539 windows)    â”‚
â”‚   â€¢ Fact Runs: 60_artifacts/enriched/j1/                        â”‚
â”‚   â€¢ Neo4j Knowledge Graph: 270 Wikidata entities, 4k topics     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ C008_CBFS: CANONICAL BIO FACTS SYSTEM                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Purpose: Provenance-first biographical memory for AI agents    â”‚
â”‚  Status: Production (v1.3) with 232 facts                       â”‚
â”‚                                                                  â”‚
â”‚  Architecture:                                                   â”‚
â”‚   â€¢ Every fact has confidence score (0.0-1.0)                   â”‚
â”‚   â€¢ Full source attribution with timestamps                     â”‚
â”‚   â€¢ Privacy tiers: public â†’ contacts â†’ private â†’ sensitive      â”‚
â”‚   â€¢ Memori taxonomy: Facts/Preferences/Skills/Rules/Context     â”‚
â”‚                                                                  â”‚
â”‚  Data Organization (13 domains):                                â”‚
â”‚   â€¢ identity: Name, birthdate, identifiers (14 facts)           â”‚
â”‚   â€¢ contact: Email, phone, addresses (3 facts)                  â”‚
â”‚   â€¢ professional: LinkedIn, work history, skills (64 facts)     â”‚
â”‚   â€¢ digital_infrastructure: Devices, repos, stack (101 facts!)  â”‚
â”‚   â€¢ relationship_networks: Family, friends (22 facts)           â”‚
â”‚   â€¢ behavioral_patterns: Decision style, communication (13)     â”‚
â”‚   â€¢ past_residences: Array of cities with temporal validity     â”‚
â”‚   â€¢ routines: Work hours, daily patterns (8 facts)              â”‚
â”‚   â€¢ health: Allergies, blood type (1 fact)                      â”‚
â”‚   â€¢ temporal_patterns: Weekly rhythms, recurring events         â”‚
â”‚   â€¢ goals: Short/long term objectives                           â”‚
â”‚   â€¢ constraints: Time, resource, energy limitations             â”‚
â”‚   â€¢ metadata: Schema tracking, update timestamps                â”‚
â”‚                                                                  â”‚
â”‚  Import Pipeline:                                                â”‚
â”‚   â€¢ Source: C003 SADB L2 database (J1 facts)                    â”‚
â”‚   â€¢ Process: 315 raw facts â†’ 232 deduplicated fields            â”‚
â”‚   â€¢ Validation: Schema v1.3 (40_src/validate_cbfs.py)           â”‚
â”‚   â€¢ Status: Ready for human verification & Betty integration    â”‚
â”‚                                                                  â”‚
â”‚  File Location: 30_config/cbfs_v1.3.yaml                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ P005_mybuddy: DIGITAL TWIN (Conversational Interface)          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Purpose: Local-first conversational AI simulating Jeremy       â”‚
â”‚  Status: Active with SADB integration                           â”‚
â”‚  Privacy: PRIVATE - No public data sharing                      â”‚
â”‚                                                                  â”‚
â”‚  Architecture:                                                   â”‚
â”‚   â€¢ RAG with Chroma vector database                             â”‚
â”‚   â€¢ Local LLMs via Ollama (no cloud dependencies)               â”‚
â”‚   â€¢ Strict provenance tracking                                  â”‚
â”‚   â€¢ Ethics guardrails built-in                                  â”‚
â”‚                                                                  â”‚
â”‚  Dual Data Pipeline:                                             â”‚
â”‚   Path 1 (Standalone): CSV chunks â†’ ingest â†’ topics â†’ index    â”‚
â”‚   Path 2 (SADB): twin_feed_v1.ndjson â†’ ingest-sadb â†’ Chroma    â”‚
â”‚                                                                  â”‚
â”‚  Key Features:                                                   â”‚
â”‚   â€¢ Interactive CLI chat (make chat)                            â”‚
â”‚   â€¢ FastAPI server on port 8000 (make api)                      â”‚
â”‚   â€¢ Evaluation harness for quality testing                      â”‚
â”‚   â€¢ Web test interface for debugging                            â”‚
â”‚                                                                  â”‚
â”‚  Data Location: $SADB_DATA_DIR (all artifacts externalized)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Additional Memory Systems

### P159_memory-system (Real-Time Conversation Memory)

**Status:** Active (last updated 2025-11-17)
**Purpose:** Real-time Claude conversation memory and session management

```
Live Claude Conversations
    â†“
CLAUDE_SESSION.md (auto-rotates @ 100KB)
    â†“
CLAUDE_CONTEXT.md (curated context)
    â†“
ChromaDB Vector Database
    â†“
MCP Memory Server (Docker)
    â†“
Obsidian Vault (knowledge management)
```

**What Makes This Different from SADB:**

| Feature | SADB (C002/C003) | P159_memory-system |
|---------|------------------|-------------------|
| **Purpose** | Historical batch processing | Real-time conversation memory |
| **Data Source** | Past conversation exports | Current Claude sessions |
| **Storage** | SQLite + structured facts | ChromaDB vector database |
| **Use Case** | Extract biographical facts | Maintain session continuity |
| **Update Frequency** | Batch runs (periodic) | Live (every conversation) |
| **Integration** | Feeds CBFS and MyBuddy | Provides context for work |

**Key Features:**
- **Automatic Session Rotation** - CLAUDE_SESSION.md rotates at 100KB
- **Semantic Search** - ChromaDB enables "What did I say about X?" queries
- **MCP Integration** - Docker-based memory server for persistence
- **Obsidian Integration** - Central knowledge vault with QA verification
- **Context Promotion** - Important decisions promoted to CLAUDE_CONTEXT.md

**Architecture Components:**
- Session management (CLAUDE_SESSION.md, CLAUDE_CONTEXT.md)
- ChromaDB vector databases (local + shared references)
- MCP memory server (Docker-based, persistent)
- Obsidian vault (structured knowledge management)
- QA verification system (Playwright-based testing)
- Claude rotation scripts (Python automation)

**Why This Exists:**
SADB processes historical conversations in batches to extract structured facts. P159 handles real-time memory during active conversations, enabling Claude to remember context across sessions without requiring manual exports and batch processing.

**Integration Points:**
- Feeds context to C002_sadb for batch processing
- Provides real-time context for W009_context_library queries
- Complements P005_mybuddy's vector search capabilities
- Future: May integrate with C009_mcp-memory-http

**Location:** `/Users/jeremybradford/SyncedProjects/P159_memory-system/`

### P190_conversation-exports-web (Metadata Extraction Pipeline)

**Status:** Active (2025-11-18 implementation)
**Purpose:** Rich metadata extraction from conversation sources

```
Anthropic Conversations + Betty (ChatGPT) Conversations
    â†“
P190_conversation-exports-web
    â†“
Structured Metadata Extraction
    â†“
[FUTURE] Merge back into SADB canonical pipeline
```

**What Makes This Special:**
- Parallel extraction system (not yet integrated with main SADB flow)
- Rich source of contextual information
- Captures metadata that standard SADB pipeline might miss
- Planned integration point with C003_sadb_canonical

**Current State:**
- Operational but standalone
- Contains valuable context not in main SADB database
- Integration strategy being developed

### P181_terminal-insights (Action Layer)

**Status:** Production, feeding B1 Braided Timeline
**Purpose:** Terminal command history with full context

```
zsh_history + Command Context
    â†“
P181_terminal-insights Processing
    â†“
Structured Command Events
    â†“
B1 Braided Timeline (in C003_sadb_canonical)
```

**Why This Matters:**
- Shows what Jeremy *did* (actions) alongside what he *discussed* (conversations)
- Provides concrete evidence of technical work
- Critical for understanding cause-effect relationships
- Enables temporal correlation: "What was I doing when I mentioned X?"

**Integration:**
- Feeds directly into C003's B1 stage
- 95,266 total events when combined with chat messages
- Time range: Sep 2023 â†’ Oct 2025
- Documentation: P181_terminal-insights/docs/SADB_INTEGRATION_BRAIDING.md

### C009_mcp-memory-http (MCP Memory Server)

**Status:** ðŸŸ¢ Production - Deployed on VPS
**Purpose:** Lightweight HTTP-based persistent key-value storage for AI agents

```
AI Agents (SADB, Mission Control, Claude Desktop)
    â†“
HTTP POST /memory/write or /memory/read
    â†“
C009 MCP Memory Server (Docker on VPS)
    â†“
memory_store.json (persistent file storage)
```

**What Makes This Different:**

| Feature | C009 MCP-HTTP | P159 Memory | SADB (C002) |
|---------|---------------|-------------|-------------|
| **Purpose** | Shared agent state | Session continuity | Biographical facts |
| **Storage** | File-based key-value | ChromaDB vectors | SQLite relational |
| **Access** | HTTP API | Direct ChromaDB | SQL queries |
| **Use Case** | Preferences, caching | Semantic search | Historical analysis |
| **Deployment** | VPS production | Local | Local + VPS |

**Architecture:**
- HTTP-based Model Context Protocol (MCP)
- Dual API: Simple HTTP endpoints + MCP JSON-RPC
- Provides `memory_write` and `memory_read` tools
- File-based storage (memory_store.json)
- Docker deployment on N8N network (internal only)
- VPS: srv999538.hstgr.cloud:8927

**Active Integrations:**
- **C002_sadb (SADB)** - Namespace: `sadb:*` (manifest tracking, extraction state)
- **C001_mission-control** - Namespaces: `houston:*`, `agents:*`, `services:*`
- **Claude Desktop** - Via mcp-stdio-bridge (STDIO â†’ HTTP adapter)

**Potential Integrations:**
- P159_memory-system (shared context across systems)
- P005_mybuddy (persistent user preferences)
- P181_terminal-insights (command history bookmarks)

**Why This Exists:**
- SADB (C002) is for structured biographical facts extraction
- P159 is for real-time conversational memory with semantic search
- C009 is for lightweight agent-to-agent state sharing
- All three serve different memory needs in the ecosystem

**Use Cases:**
- Agent status tracking (what Houston is doing)
- Build timestamps (when SADB last ran)
- User preferences (theme, display settings)
- Cache data (frequently accessed values)
- Cross-agent communication (shared state)

**Not Suitable For:**
- Large data storage (use SADB)
- Semantic search (use P159 ChromaDB)
- High-volume transactions (file-based, not optimized for throughput)

**Location:** `/Users/jeremybradford/SyncedProjects/C009_mcp-memory-http/`
**Production URL:** http://srv999538.hstgr.cloud:8927 (internal N8N network only)

### P159_memory-system (ChromaDB Memory)

**Status:** Active
**Purpose:** ChromaDB-based persistent memory system

**Features:**
- Works with Claude and local LLMs
- Vector similarity search for context retrieval
- Alternative to SADB for lightweight memory needs
- More immediate than SADB's batch pipeline

---

## Work Systems Integration

### How Memory Feeds Analytics

Several work projects depend on SADB data for context and insights:

#### W001_cmo-weekly-reporting
**Dependency:** C002_sadb
**How:** Queries SADB for personal context to enrich marketing analytics
**Example:** "Mention if Jeremy discussed competitor analysis this week"

#### W009_context_library
**Role:** "Front desk for knowledge access"
**Dependency:** C002_sadb, potentially CBFS
**How:** Central query point for retrieving contextual information
**Purpose:** Provides quick access to relevant memories for work tasks

#### Other Work Projects (W002-W012)
Most work projects are standalone but may benefit from:
- Personal context from CBFS (understanding Jeremy's perspective)
- Historical decisions from SADB (why choices were made)
- Timeline correlation from B1 (when events happened)

---

## The SADB Ecosystem (Sprawling - Needs Consolidation)

âš ï¸ **This section documents a known problem area requiring cleanup.**

### What Exists

Current SADB-related folders and their purposes:

| Location | Purpose | Status | Cleanup Need |
|----------|---------|--------|--------------|
| C002_sadb | Core processing pipeline | âœ… Active | Low - well-defined |
| C003_sadb_canonical | Refinement stages (S0-B1) | âœ… Active | Low - documented |
| $SADB_DATA_DIR | External data directory | âœ… Active | Medium - some old runs |
| SADB_backups/ | Various backup locations | âš ï¸ Scattered | High - consolidate |
| Multiple extraction runs | Historical test runs | âš ï¸ Unclear | High - archive old runs |

### The Problem

**Symptoms:**
- Hard to tell what's current vs historical vs test data
- Multiple directories with "SADB" in the name
- Old extraction runs mixed with current ones
- Unclear which outputs are canonical

**Impact on LLMs:**
- May grab outdated fact extraction runs
- Confusion about which SADB repo to use (C002 vs C003)
- Difficulty determining "ground truth" data location

### The Solution (Planned)

**Phase 1: Documentation**
- Mark current/canonical files explicitly in README files
- Add "DEPRECATED" warnings to old runs
- Create clear date stamps on extraction directories

**Phase 2: Archival**
- Move old extraction runs to `90_archive/extractions_YYYY/`
- Consolidate scattered backups
- Create manifest of what was archived and why

**Phase 3: Guardrails**
- Pre-commit hooks to prevent new scattered directories
- Naming conventions enforced (runs_YYYYMMDD_HHMMSS format)
- Automatic cleanup scripts for >90 day old test runs

---

## Critical Dependencies

### Execution Order (Must Follow This Sequence)

When working with memory systems, respect these dependencies:

1. **C002_sadb** must run first
   - Creates canonical SQLite store
   - Generates twin_feed_v1.ndjson
   - All downstream systems depend on this output

2. **C003_sadb_canonical** refines C002 output
   - Cannot run without C002 data
   - Stages S0-S2 prepare data for M1-B1
   - M1, U1, J1 depend on S1/S2 completion
   - B1 requires both S1 data AND P181 terminal data

3. **C008_CBFS** imports from C003
   - Specifically depends on J1 fact extraction
   - Requires C003 L2 database to exist
   - Import pipeline: C003 J1 â†’ CBFS YAML

4. **P005_mybuddy** queries twin feed OR CBFS
   - Can use C002 twin feed directly (path 2)
   - Can use standalone CSV (path 1)
   - Future: May query CBFS for richer context

5. **Other systems** query SADB or CBFS as needed
   - W001, W009 query C002_sadb
   - P190 operates in parallel (not dependent)
   - P181 feeds into C003 but doesn't depend on it

### Dependency Graph

```
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚  Conversations   â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚   C002_sadb      â”‚
                   â”‚  (core pipeline) â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚                       â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚ C003_sadb        â”‚    â”‚ W001, W009     â”‚
       â”‚ (refinement)     â”‚    â”‚ (work queries) â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚  C008_CBFS  â”‚   â”‚ P005_mybuddyâ”‚
â”‚  (facts)    â”‚   â”‚ (twin)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    [P181 terminal data] â”€â”€â”€â”€â”€â”€â”€â”
                                 â”‚
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚  C003 B1 Stage   â”‚
                        â”‚ (braided timeline)â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    [P190 metadata extraction]  (parallel, future integration)
```

---

## For LLMs: Finding Canonical Versions

### The Problem We're Solving

LLMs frequently make these mistakes:
1. âŒ Grabbing test scripts instead of production code
2. âŒ Using outdated extraction runs instead of current ones
3. âŒ Confusing C002_sadb with C003_sadb_canonical
4. âŒ Working on archived projects instead of active ones

### Solution: Always Follow This Checklist

#### Step 1: Check Project Status
```bash
# Read this first!
cat ~/SyncedProjects/C000_info-center/workspace/KNOWN_PROJECTS.md
```

Look for:
- Last modified date (is it recent?)
- Status indicators (Active? Archived?)
- Project ID (C### for core, P### for personal)

#### Step 2: Read the Project's CLAUDE.md
```bash
# This file exists in EVERY active project
cat ~/SyncedProjects/PROJECT_NAME/CLAUDE.md
```

Contains:
- Quick start commands
- Canonical file paths
- Common pitfalls
- Build instructions

#### Step 3: Understand Multi-Version Scenarios

**SADB Disambiguation:**
- `C002_sadb` = Core processing, creates SQLite + twin feed
- `C003_sadb_canonical` = Refinement pipeline, stages S0-B1
- When in doubt: C003 is the newer, more sophisticated version
- But C002 must run BEFORE C003 can process its output

**Rule:** If you need fact extraction (J1), use C003. If you need basic ingestion, use C002.

#### Step 4: Look for Temporal Markers

**Good file naming:**
```
runs_gptoss_20251104_235333/     â† Timestamp shows this is Nov 4, 2025
facts_llm_gptoss_merged.kept.jsonl
```

**Bad file naming:**
```
test_output/         â† No date, unclear status
results/             â† Generic, could be anything
```

**Rule:** Prefer files/folders with ISO timestamps (YYYYMMDD_HHMMSS)

#### Step 5: Check for "Canonical" Markers

Look for these indicators:
- "canonical" in folder/file names
- "current" or "latest" symlinks
- "DEPRECATED" or "ARCHIVED" warnings
- Date stamps in README files

#### Step 6: Verify with Last Modified Dates

```bash
# When choosing between similar files
ls -lt ~/SyncedProjects/C003_sadb_canonical/60_artifacts/enriched/j1/

# Most recent is usually (but not always) canonical
# Cross-reference with project's CLAUDE.md to be sure
```

### Common Confusion Points (Reference Guide)

| User Says | Might Mean | Check This |
|-----------|------------|------------|
| "Work on SADB" | C002? C003? Both? | KNOWN_PROJECTS.md + user intent |
| "Use twin feed" | C002 output OR P005 input? | Context of conversation |
| "Latest facts" | J1 stage? J2? J3? | C003 CLAUDE.md canonical paths |
| "Memory system" | SADB? CBFS? MyBuddy? MCP? | What type of memory operation? |
| "Terminal data" | P181 output OR B1 integration? | User's specific need |

### Decision Tree for File Selection

```
START: User asks to "work on X"
â”‚
â”œâ”€ Is X mentioned in KNOWN_PROJECTS.md?
â”‚  â”œâ”€ NO â†’ Ask user for clarification
â”‚  â””â”€ YES â†’ Continue
â”‚
â”œâ”€ Is project status "Active"?
â”‚  â”œâ”€ NO (Archived/Maintenance) â†’ Confirm with user before proceeding
â”‚  â””â”€ YES â†’ Continue
â”‚
â”œâ”€ Does CLAUDE.md specify canonical paths?
â”‚  â”œâ”€ YES â†’ Use those exact paths
â”‚  â””â”€ NO â†’ Check for timestamps/date markers
â”‚
â”œâ”€ Multiple versions exist?
â”‚  â”œâ”€ NO â†’ Use the one available
â”‚  â””â”€ YES â†’ Prefer: "canonical" name > most recent date > ask user
â”‚
â””â”€ SUCCESS: You have the right file!
```

### When to Ask Instead of Guess

**Always ask if:**
- Multiple projects have similar names (C002 vs C003)
- Files/folders lack date stamps
- Status is unclear (Active vs Archived)
- User says "the usual file" but you don't know which one
- Last modified dates are all old (maybe project is stale?)

**Better to ask:** "Should I use C002_sadb or C003_sadb_canonical for this task?"
**Than to guess wrong** and waste 20 minutes on the wrong pipeline.

---

## Maintenance Notes

### Update Triggers

This document should be updated when:
- New memory systems are added (e.g., M2/M3 stages implemented)
- Pipeline stages change (S0-S2 evolve)
- Dependencies shift (new integration points)
- Cleanup initiatives complete (SADB consolidation)

### Document Owners

- **Memory Architecture:** Jeremy Bradford
- **Technical Details:** Claude (with Jeremy's review)
- **Work Integration:** Jeremy Bradford
- **Canonical Paths:** Auto-discovered + manually verified

### Version History

- 2025-11-19: Initial creation with S0-S2, M1, U1, J1, B1 stages documented
- [Future]: M2/M3 stages to be added when implemented
- [Future]: P190 integration pathway when finalized

---

*Last Updated: 2025-11-19*
*Location: C000_info-center/workspace/PROJECT_RELATIONSHIPS.md*
*Maintained by: Jeremy Bradford & Claude*
